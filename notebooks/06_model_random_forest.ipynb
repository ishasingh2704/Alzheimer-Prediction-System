{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e71b88b",
   "metadata": {},
   "source": [
    "# Random Forest Model\n",
    "\n",
    "This notebook implements and evaluates a Random Forest classifier for Alzheimer's disease prediction.\n",
    "\n",
    "## Key Features:\n",
    "- **Algorithm**: Random Forest ensemble method with multiple decision trees\n",
    "- **Hyperparameter Tuning**: GridSearchCV with cross-validation\n",
    "- **Feature Importance**: Built-in feature importance analysis\n",
    "- **Evaluation**: Comprehensive metrics, confusion matrices, and ROC curves\n",
    "\n",
    "## Model Overview:\n",
    "Random Forest combines multiple decision trees using bootstrap sampling and random feature selection. This reduces overfitting and provides robust predictions with built-in feature importance rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d203b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    RocCurveDisplay, ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e852d",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Loading the preprocessed and split dataset from the preprocessing phase. The data includes:\n",
    "- **Training set**: Used for model training and hyperparameter tuning\n",
    "- **Validation set**: Used for model evaluation during development  \n",
    "- **Test set**: Final evaluation on unseen data\n",
    "\n",
    "All features have been preprocessed, encoded, and are ready for ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "X_train = joblib.load('../outputs/preprocessed/X_train.pkl')\n",
    "X_val = joblib.load('../outputs/preprocessed/X_val.pkl')\n",
    "X_test = joblib.load('../outputs/preprocessed/X_test.pkl')\n",
    "y_train = joblib.load('../outputs/preprocessed/y_train.pkl')\n",
    "y_val = joblib.load('../outputs/preprocessed/y_val.pkl')\n",
    "y_test = joblib.load('../outputs/preprocessed/y_test.pkl')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6251441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, model_name=\"Model\"):\n",
    "    start_pred = time.time()\n",
    "    y_pred = model.predict(X_val)\n",
    "    end_pred = time.time()\n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "        \"Precision\": precision_score(y_val, y_pred),\n",
    "        \"Recall\": recall_score(y_val, y_pred),\n",
    "        \"F1 Score\": f1_score(y_val, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_val, y_pred),\n",
    "        \"Prediction Time (s)\": end_pred - start_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Results:\")\n",
    "    print(pd.DataFrame(metrics, index=[0]))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Alzheimer', 'Alzheimer'])\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_estimator(model, X_val, y_val)\n",
    "    plt.title(f'{model_name} ROC Curve')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef9951",
   "metadata": {},
   "source": [
    "## 2. Training Phase - Random Forest Model\n",
    "\n",
    "### Hyperparameter Tuning Strategy:\n",
    "- **n_estimators**: Number of trees in the forest (100, 200) - more trees = better performance but longer training\n",
    "- **max_depth**: Maximum depth of trees (10, 20, None) - controls overfitting vs underfitting\n",
    "- **min_samples_split**: Minimum samples to split a node (2, 5) - prevents overfitting\n",
    "- **min_samples_leaf**: Minimum samples in leaf nodes (1, 2) - smooths decision boundaries\n",
    "\n",
    "### Random Forest Benefits:\n",
    "1. **Ensemble Learning**: Combines predictions from multiple decision trees\n",
    "2. **Feature Randomness**: Each tree uses random subset of features\n",
    "3. **Bootstrap Sampling**: Each tree trained on random sample of data\n",
    "4. **Built-in Feature Importance**: No separate analysis needed\n",
    "\n",
    "GridSearchCV with 5-fold cross-validation finds the optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce069b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with GridSearch\n",
    "print(\"Training Random Forest with GridSearchCV...\")\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_grid.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Get best model\n",
    "best_rf = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "y_pred_val, metrics_val = evaluate_model(best_rf, X_val, y_val, \"Random Forest (Validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61dd24d",
   "metadata": {},
   "source": [
    "## 3. Testing Phase - Random Forest Model\n",
    "\n",
    "### Final Model Evaluation:\n",
    "After selecting the best hyperparameters through cross-validation, we evaluate the model on the held-out test set to get unbiased performance estimates.\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **Accuracy**: Overall classification correctness\n",
    "- **Precision**: Of predicted Alzheimer cases, how many are actually positive\n",
    "- **Recall**: Of actual Alzheimer cases, how many were correctly identified  \n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under the ROC curve, measures discrimination ability\n",
    "\n",
    "### Visualization:\n",
    "- **Confusion Matrix**: Shows true vs predicted classifications\n",
    "- **ROC Curve**: Plots true positive rate vs false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8597f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "y_proba_test = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest - Test Set Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_test))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f539a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for test set\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Alzheimer', 'Alzheimer'])\n",
    "disp.plot(cmap='Greens', values_format='d')\n",
    "plt.title('Random Forest Test Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for test set\n",
    "RocCurveDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title('Random Forest Test Set ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d1b2d",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "Random Forest provides built-in feature importance based on how much each feature decreases impurity when used for splits across all trees. Higher importance values indicate more discriminative features for Alzheimer's prediction.\n",
    "\n",
    "The top features represent the most influential clinical indicators in the classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 15 Feature Importances - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b708a95",
   "metadata": {},
   "source": [
    "## 4. Model Persistence\n",
    "\n",
    "Saving the trained Random Forest model for future use in predictions or model comparisons. The saved model includes:\n",
    "- Best hyperparameters found through GridSearchCV\n",
    "- Trained ensemble of decision trees\n",
    "- Feature importance rankings\n",
    "- Complete model ready for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to outputs/models folder\n",
    "os.makedirs('../outputs/models', exist_ok=True)\n",
    "model_path = '../outputs/models/random_forest_model.pkl'\n",
    "joblib.dump(best_rf, model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1c993",
   "metadata": {},
   "source": [
    "## 5. Final Results Summary\n",
    "\n",
    "The Random Forest model performance summary for easy comparison with other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db416fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions and metrics\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred_test,\n",
    "    'Prediction_Probability': y_proba_test\n",
    "})\n",
    "\n",
    "# Display final metrics for model comparison\n",
    "print(\"=\"*50)\n",
    "print(\"RANDOM FOREST MODEL - FINAL PERFORMANCE SUMMARY\")  \n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_proba_test):.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create results dataframe for display  \n",
    "metrics_summary = {\n",
    "    'Model': ['Random Forest'],\n",
    "    'Accuracy': [accuracy_score(y_test, y_pred_test)],\n",
    "    'Precision': [precision_score(y_test, y_pred_test)], \n",
    "    'Recall': [recall_score(y_test, y_pred_test)],\n",
    "    'F1_Score': [f1_score(y_test, y_pred_test)],\n",
    "    'ROC_AUC': [roc_auc_score(y_test, y_proba_test)]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(metrics_df.round(4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
