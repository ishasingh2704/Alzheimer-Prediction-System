{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170aba8f",
   "metadata": {},
   "source": [
    "# Model 3 — Random Forest Classifier\n",
    "\n",
    "This notebook trains a Random Forest classifier with GridSearchCV, evaluates performance (accuracy, precision, recall, F1, ROC-AUC), plots confusion matrix and ROC curve, visualizes feature importance, saves the best model to `models/random_forest_best.pkl`, and appends metrics to a model comparison file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, classification_report, roc_curve)\n",
    "\n",
    "# Configure plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('../outputs/models', exist_ok=True)\n",
    "os.makedirs('../outputs/metrics', exist_ok=True)\n",
    "os.makedirs('..\\models', exist_ok=True)  # repo-level models folder\n",
    "\n",
    "print('✅ Imports and output directories ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1806f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load preprocessed data\n",
    "# The preprocessing notebook saved pickles to ../outputs/preprocessed/*.pkl\n",
    "from pathlib import Path\n",
    "data_dir = Path('../outputs/preprocessed')\n",
    "candidates = {\n",
    "    'X_train': data_dir / 'X_train.pkl',\n",
    "    'X_val': data_dir / 'X_val.pkl',\n",
    "    'X_test': data_dir / 'X_test.pkl',\n",
    "    'y_train': data_dir / 'y_train.pkl',\n",
    "    'y_val': data_dir / 'y_val.pkl',\n",
    "    'y_test': data_dir / 'y_test.pkl'\n",
    "}\n",
    "loaded = {}\n",
    "for name, path in candidates.items():\n",
    "    if path.exists():\n",
    "        loaded[name] = joblib.load(path)\n",
    "        print(f'Loaded {name} from {path}')\n",
    "    else:\n",
    "        print(f'Warning: {path} not found. Attempting fallback to ../outputs/preprocessed/{name}.pkl')\n",
    "        # keep going; user may need to run preprocessing first\n",
    "\n",
    "# Quick check that required objects loaded\n",
    "required = ['X_train','X_val','X_test','y_train','y_val','y_test']\n",
    "missing = [r for r in required if r not in loaded]\n",
    "if missing:\n",
    "    print('Some preprocessed datasets are missing:', missing)\n",
    "    print('Please run the preprocessing notebook (02_data_preprocessing.ipynb) or move the pickles to ../outputs/preprocessed/')\n",
    "else:\n",
    "    X_train = loaded['X_train']\n",
    "    X_val = loaded['X_val']\n",
    "    X_test = loaded['X_test']\n",
    "    y_train = loaded['y_train']\n",
    "    y_val = loaded['y_val']\n",
    "    y_test = loaded['y_test']\n",
    "    print(f'✅ Data shapes — X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889204a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Evaluation helper\n",
    "def evaluate_model(model, X, y, labels=None, prefix='val'):\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        # try predict_proba for ROC AUC; if not available, try decision_function\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X)[:, 1]\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            y_proba = model.decision_function(X)\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = float(accuracy_score(y, y_pred))\n",
    "    metrics['precision'] = float(precision_score(y, y_pred, zero_division=0))\n",
    "    metrics['recall'] = float(recall_score(y, y_pred, zero_division=0))\n",
    "    metrics['f1'] = float(f1_score(y, y_pred, zero_division=0))\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            metrics['roc_auc'] = float(roc_auc_score(y, y_proba))\n",
    "        except Exception:\n",
    "            metrics['roc_auc'] = None\n",
    "    else:\n",
    "        metrics['roc_auc'] = None\n",
    "\n",
    "    # Confusion matrix plot\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix — {prefix}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    cm_path = f'../outputs/models/confusion_matrix_{prefix}.png'\n",
    "    plt.savefig(cm_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # ROC curve\n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {metrics.get(\n",
    "):.3f}' if metrics.get('roc_auc') else 'ROC')\n",
    "        plt.plot([0,1],[0,1],'k--', alpha=0.6)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve — {prefix}')\n",
    "        plt.legend(loc='lower right')\n",
    "        roc_path = f'../outputs/models/roc_curve_{prefix}.png'\n",
    "        plt.savefig(roc_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        roc_path = None\n",
    "\n",
    "    # Classification report (string)\n",
    "    metrics['classification_report'] = classification_report(y, y_pred, zero_division=0, output_dict=True)\n",
    "    metrics['confusion_matrix_path'] = cm_path\n",
    "    metrics['roc_curve_path'] = roc_path\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print('✅ Evaluation helper defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e86755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print('✅ Parameter grid ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6bf549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-8) GridSearchCV and training on training set, evaluation on validation set\n",
    "if 'X_train' in globals():\n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "    grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    print('Starting GridSearchCV (this may take some time) ...')\n",
    "    start = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(f'GridSearchCV done. Time elapsed: {(end-start)/60:.2f} minutes')\n",
    "\n",
    "    print('Best parameters:')\n",
    "    print(grid.best_params_)\n",
    "    print(f'Best CV score: {grid.best_score_:.4f}')\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_metrics = evaluate_model(best_model, X_val, y_val, prefix='validation')\n",
    "    print('Validation metrics:')\n",
    "    for k,v in val_metrics.items():\n",
    "        if k not in ['classification_report','confusion_matrix_path','roc_curve_path']:\n",
    "            print(f'  {k}: {v}')\n",
    "\n",
    "    # Evaluate on test set as well (final evaluation)\n",
    "    test_metrics = evaluate_model(best_model, X_test, y_test, prefix='test')\n",
    "    print('Test metrics:')\n",
    "    for k,v in test_metrics.items():\n",
    "        if k not in ['classification_report','confusion_matrix_path','roc_curve_path']:\n",
    "            print(f'  {k}: {v}')\n",
    "\n",
    "    # 9) Feature importance (top 20)\n",
    "    try:\n",
    "        importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
    "        top20 = importances.sort_values(ascending=False).head(20)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.barplot(x=top20.values, y=top20.index, palette='viridis')\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.xlabel('Importance')\n",
    "        fi_path = '../outputs/models/feature_importances_top20.png'\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fi_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f'Feature importance plot saved to {fi_path}')\n",
    "    except Exception as e:\n",
    "        print('Could not compute feature importances:', e)\n",
    "\n",
    "    # OOB score if available\n",
    "    oob = getattr(best_model, 'oob_score_', None)\n",
    "    print(f'OOB score: {oob}')\n",
    "\n",
    "    # 10) Save the trained model\n",
    "    model_path = 'models/random_forest_best.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f'Best model saved to {model_path}')\n",
    "\n",
    "    # 11) Append metrics to global results JSON for model comparison\n",
    "    results_path = '../outputs/models/model_comparison.json'\n",
    "    all_results = {}\n",
    "    if os.path.exists(results_path):\n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                all_results = json.load(f)\n",
    "        except Exception:\n",
    "            all_results = {}\n",
    "\n",
    "    model_name = 'RandomForest_GridSearch'\n",
    "    all_results[model_name] = {\n",
    "        'best_params': grid.best_params_,\n",
    "        'best_cv_score': grid.best_score_,\n",
    "        'validation_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'oob_score': oob,\n",
    "        'saved_model_path': model_path,\n",
    "        'feature_importance_plot': fi_path if 'fi_path' in locals() else None,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "    print(f'Metrics appended to {results_path}')\n",
    "else:\n",
    "    print('Preprocessed data not loaded; cannot run training. Please run preprocessing first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3cf885",
   "metadata": {},
   "source": [
    "## Notes / Explanations\n",
    "\n",
    "### Why Random Forest?\n",
    "Random Forests are robust, handle mixed data types, resist overfitting through ensembling, provide feature importance measures, and generally perform well out-of-the-box for tabular health data.\n",
    "\n",
    "### Hyperparameters tuned\n",
    "- n_estimators: Number of trees in the forest. More trees can improve performance but increase training time.\n",
    "- max_depth: Maximum depth of each tree. Controls complexity; None means nodes expanded until pure or min_samples_split.\n",
    "- min_samples_split: Minimum number of samples required to split an internal node. Larger values prevent small splits and can reduce overfitting.\n",
    "- max_features: Number of features to consider when looking for the best split. 'sqrt' and 'log2' are common choices that reduce correlation between trees.\n",
    "\n",
    "### Insights from Feature Importance\n",
    "The top features (shown in the saved plot) indicate which clinical, cognitive, or lifestyle variables the model relies on most. These insights can guide feature selection and clinical interpretation.\n",
    "\n",
    "---\n",
    "*Next steps:* review the generated `model_comparison.json`, inspect the saved plots in `../outputs/models/`, and, if desired, run a lighter grid (fewer combinations) for faster experimentation or use RandomizedSearchCV for larger hyperparameter spaces."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
